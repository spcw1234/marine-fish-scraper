"""
Main scraping engine that coordinates all scrapers and components
"""

import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

# í˜„ì¬ í´ë”ì˜ ëª¨ë“ˆë“¤ import (ìƒëŒ€ ê²½ë¡œ ì œê±°)
try:
    from scraping_session import ScrapingSession, SessionManager, SessionStatus
    from image_metadata import ImageMetadata, MetadataCollection
    from config_manager import ConfigManager
    from taxonomy_manager import TaxonomyManager
    from image_downloader import ImageDownloader, DownloadResult
    from image_validator import ImageValidator
    from logger import get_logger
    from error_handler import get_error_handler, with_retry, ErrorType
except ImportError as e:
    print(f"Import error: {e}")
    print("ì¼ë¶€ ëª¨ë“ˆì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")

# ìŠ¤í¬ë˜í¼ ì„í¬íŠ¸ (í˜„ì¬ëŠ” ì£¼ì„ ì²˜ë¦¬)
# from fishbase_scraper import FishBaseScraper
# from google_images_scraper import GoogleImagesScraper
# from reef2reef_scraper import Reef2ReefScraper
# from flickr_scraper import FlickrScraper


@dataclass
class ScrapingResult:
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼"""
    genus: str
    species: str
    total_found: int
    total_downloaded: int
    source_results: Dict[str, int]
    errors: List[str]
    duration: float


class ScraperCore:
    """ë©”ì¸ ìŠ¤í¬ë˜í•‘ ì—”ì§„"""
    
    def __init__(self, config_path: Optional[str] = None, base_dir: str = "."):
        # ì„¤ì • ë° ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.config = ConfigManager(config_path)
        self.logger = get_logger("scraper_core")
        self.error_handler = get_error_handler()
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.base_dir = Path(base_dir)
        self.dataset_dir = self.base_dir / "dataset"
        self.sessions_dir = self.base_dir / "sessions"
        self.metadata_dir = self.base_dir / "metadata"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for directory in [self.base_dir, self.dataset_dir, self.sessions_dir, self.metadata_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.taxonomy_manager = TaxonomyManager()
        self.session_manager = SessionManager(str(self.sessions_dir))
        self.image_validator = ImageValidator()
        self.image_downloader = ImageDownloader(self.config)
        
        # ìŠ¤í¬ë˜í¼ë“¤ ì´ˆê¸°í™”
        self.scrapers = self._initialize_scrapers()
        
        # ë©”íƒ€ë°ì´í„° ì»¬ë ‰ì…˜
        self.metadata_collection = MetadataCollection()
        
        # í†µê³„
        self.scraping_stats = {
            'total_species_processed': 0,
            'total_images_found': 0,
            'total_images_downloaded': 0,
            'total_errors': 0,
            'source_stats': {},
            'start_time': None,
            'end_time': None
        }
    
    def _initialize_scrapers(self) -> Dict[str, Any]:
        """ìŠ¤í¬ë˜í¼ë“¤ ì´ˆê¸°í™”"""
        scrapers = {}
        
        enabled_sources = self.config.get_enabled_sources()
        
        if 'fishbase' in enabled_sources:
            scrapers['fishbase'] = FishBaseScraper(self.config)
            
        if 'google_images' in enabled_sources:
            scrapers['google_images'] = GoogleImagesScraper(self.config)
            
        if 'reef2reef' in enabled_sources:
            scrapers['reef2reef'] = Reef2ReefScraper(self.config)
            
        if 'flickr' in enabled_sources:
            scrapers['flickr'] = FlickrScraper(self.config)
        
        self.logger.info(f"í™œì„±í™”ëœ ìŠ¤í¬ë˜í¼: {list(scrapers.keys())}")
        return scrapers
    
    def scrape_all_species(self, resume_session: Optional[str] = None) -> ScrapingSession:
        """ëª¨ë“  ì¢… ìŠ¤í¬ë˜í•‘"""
        self.logger.info("ğŸš€ ì „ì²´ ì¢… ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        # ì„¸ì…˜ ìƒì„± ë˜ëŠ” ë³µì›
        if resume_session:
            session = self.session_manager.load_session(resume_session)
            if not session:
                self.logger.error(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {resume_session}")
                return None
            session.resume()
            self.logger.info(f"ì„¸ì…˜ ë³µì›: {session.session_id}")
        else:
            # ëª¨ë“  ì¢… ëª©ë¡ ìƒì„±
            all_species = []
            for species_info in self.taxonomy_manager.get_all_species():
                all_species.append(species_info.scientific_name)
            
            session = self.session_manager.create_session(all_species, self.config.to_dict())
            session.start()
        
        self.session_manager.set_active_session(session.session_id)
        
        try:
            # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            self._execute_scraping_session(session)
            
            # ì„¸ì…˜ ì™„ë£Œ
            session.complete()
            self.logger.info(f"âœ… ì „ì²´ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {session.session_id}")
            
        except KeyboardInterrupt:
            session.pause()
            self.logger.info(f"â¸ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¼ì‹œì •ì§€: {session.session_id}")
            
        except Exception as e:
            session.fail(str(e))
            self.logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            
        finally:
            self.session_manager.save_session(session)
        
        return session
    
    def scrape_specific_family(self, class_name: str, order_name: str, family_name: str) -> ScrapingSession:
        """íŠ¹ì • ê³¼ ìŠ¤í¬ë˜í•‘"""
        self.logger.info(f"ğŸ¯ {family_name} ê³¼ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        # í•´ë‹¹ ê³¼ì˜ ì¢… ëª©ë¡ ìƒì„±
        species_list = []
        family_species = self.taxonomy_manager.get_species_by_family(class_name, order_name, family_name)
        
        for genus, species in family_species:
            species_list.append(f"{genus} {species}")
        
        if not species_list:
            self.logger.error(f"âŒ {family_name} ê³¼ì—ì„œ ì¢…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # ì„¸ì…˜ ìƒì„±
        session = self.session_manager.create_session(species_list, self.config.to_dict())
        session.start()
        
        try:
            self._execute_scraping_session(session)
            session.complete()
            self.logger.info(f"âœ… {family_name} ê³¼ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
            
        except KeyboardInterrupt:
            session.pause()
            self.logger.info("â¸ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¼ì‹œì •ì§€")
            
        except Exception as e:
            session.fail(str(e))
            self.logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            
        finally:
            self.session_manager.save_session(session)
        
        return session
    
    def _execute_scraping_session(self, session: ScrapingSession):
        """ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ì‹¤í–‰"""
        remaining_species = session.get_remaining_species()
        total_species = len(remaining_species)
        
        self.logger.info(f"ğŸ“Š ì²˜ë¦¬í•  ì¢… ìˆ˜: {total_species}")
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        max_workers = min(self.config.scraping.concurrent_downloads, 3)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì¢…ë³„ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì œì¶œ
            future_to_species = {}
            
            for i, species_name in enumerate(remaining_species):
                if session.status != SessionStatus.RUNNING:
                    break
                
                genus, species = species_name.split(' ', 1)
                
                future = executor.submit(self._scrape_single_species, genus, species, session)
                future_to_species[future] = species_name
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (i + 1) % 10 == 0:
                    progress = (i + 1) / total_species * 100
                    self.logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {i + 1}/{total_species} ({progress:.1f}%)")
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_species):
                species_name = future_to_species[future]
                
                try:
                    result = future.result()
                    if result:
                        self.logger.info(f"âœ… {species_name}: {result.total_downloaded}ì¥ ë‹¤ìš´ë¡œë“œ")
                    else:
                        self.logger.warning(f"âš ï¸ {species_name}: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
                        
                except Exception as e:
                    self.logger.error(f"âŒ {species_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    genus, species = species_name.split(' ', 1)
                    session.mark_species_failed(genus, species, "unknown", str(e))
                
                # ì„¸ì…˜ ì €ì¥ (ì£¼ê¸°ì )
                self.session_manager.save_session(session)
    
    @with_retry(error_types=[ErrorType.NETWORK, ErrorType.FILE_SYSTEM])
    def _scrape_single_species(self, genus: str, species: str, session: ScrapingSession) -> Optional[ScrapingResult]:
        """ë‹¨ì¼ ì¢… ìŠ¤í¬ë˜í•‘"""
        start_time = time.time()
        
        try:
            # ì¢… ì •ë³´ ì¡°íšŒ
            species_info = self.taxonomy_manager.get_species_info(genus, species)
            if not species_info:
                self.logger.warning(f"âš ï¸ ì¢… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {genus} {species}")
                return None
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            save_dir = (self.dataset_dir / species_info.class_name / 
                       species_info.order / species_info.family / 
                       f"{genus}_{species}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ê° ì†ŒìŠ¤ì—ì„œ ì´ë¯¸ì§€ URL ìˆ˜ì§‘
            all_image_urls = []
            source_results = {}
            errors = []
            
            enabled_sources = self.config.get_enabled_sources()
            
            for source_name, scraper in self.scrapers.items():
                if source_name not in enabled_sources:
                    continue
                
                try:
                    source_config = enabled_sources[source_name]
                    max_images = source_config.max_images
                    
                    self.logger.debug(f"ğŸ” {source_name}ì—ì„œ {genus} {species} ê²€ìƒ‰ ì¤‘...")
                    
                    # ì†ŒìŠ¤ë³„ ì´ë¯¸ì§€ URL ìˆ˜ì§‘
                    if source_name == 'fishbase':
                        image_urls = scraper.search_species_images(genus, species, max_images)
                    else:
                        common_names = species_info.common_names
                        image_urls = scraper.search_species_images(genus, species, common_names, max_images)
                    
                    source_results[source_name] = len(image_urls)
                    all_image_urls.extend(image_urls)
                    
                    self.logger.debug(f"ğŸ“¥ {source_name}: {len(image_urls)}ê°œ URL ìˆ˜ì§‘")
                    
                    # ì†ŒìŠ¤ ê°„ ì§€ì—°
                    time.sleep(self.config.scraping.delay_between_requests)
                    
                except Exception as e:
                    error_msg = f"{source_name} ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
                    errors.append(error_msg)
                    source_results[source_name] = 0
                    session.mark_species_failed(genus, species, source_name, error_msg)
                    self.logger.warning(f"âš ï¸ {error_msg}")
            
            # ì¤‘ë³µ URL ì œê±°
            unique_urls = []
            seen_urls = set()
            
            for img_data in all_image_urls:
                url = img_data['url']
                if url not in seen_urls:
                    seen_urls.add(url)
                    unique_urls.append(img_data)
            
            total_found = len(unique_urls)
            self.logger.info(f"ğŸ” {genus} {species}: ì´ {total_found}ê°œ ê³ ìœ  ì´ë¯¸ì§€ ë°œê²¬")
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            downloaded_count = 0
            
            for i, img_data in enumerate(unique_urls):
                try:
                    filename_base = f"{img_data['source']}_{genus}_{species}_{i+1:03d}"
                    
                    result = self.image_downloader.download_image(
                        url=img_data['url'],
                        save_dir=save_dir,
                        filename_base=filename_base,
                        genus=genus,
                        species=species,
                        source=img_data['source'],
                        common_names=species_info.common_names
                    )
                    
                    if result.success:
                        downloaded_count += 1
                        
                        # ë©”íƒ€ë°ì´í„° ì»¬ë ‰ì…˜ì— ì¶”ê°€
                        if result.metadata:
                            self.metadata_collection.add(result.metadata)
                        
                        # ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        session.update_species_progress(genus, species, 1, img_data['source'])
                    
                    # ë‹¤ìš´ë¡œë“œ ê°„ ì§€ì—°
                    time.sleep(self.config.scraping.delay_between_requests * 0.5)
                    
                except Exception as e:
                    error_msg = f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_data['url']} - {str(e)}"
                    errors.append(error_msg)
                    self.logger.debug(f"âš ï¸ {error_msg}")
            
            # ê²°ê³¼ ìƒì„±
            duration = time.time() - start_time
            
            result = ScrapingResult(
                genus=genus,
                species=species,
                total_found=total_found,
                total_downloaded=downloaded_count,
                source_results=source_results,
                errors=errors,
                duration=duration
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {genus} {species} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            session.mark_species_failed(genus, species, "unknown", str(e))
            return None
    
    def _update_stats(self, result: ScrapingResult):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.scraping_stats['total_species_processed'] += 1
        self.scraping_stats['total_images_found'] += result.total_found
        self.scraping_stats['total_images_downloaded'] += result.total_downloaded
        self.scraping_stats['total_errors'] += len(result.errors)
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        for source, count in result.source_results.items():
            if source not in self.scraping_stats['source_stats']:
                self.scraping_stats['source_stats'][source] = {'found': 0, 'downloaded': 0}
            self.scraping_stats['source_stats'][source]['found'] += count
    
    def create_training_dataset(self, images_per_class: int = 100, 
                              output_dir: Optional[str] = None) -> bool:
        """í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±"""
        self.logger.info(f"ğŸ¯ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ (í´ë˜ìŠ¤ë‹¹ {images_per_class}ì¥)")
        
        if output_dir:
            train_dir = Path(output_dir)
        else:
            train_dir = self.base_dir / "train"
        
        train_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # í´ë˜ìŠ¤ë³„ í´ë” ìƒì„±
            for class_name in self.taxonomy_manager.fish_taxonomy.keys():
                class_dir = train_dir / class_name
                class_dir.mkdir(exist_ok=True)
                
                # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘
                class_dataset_dir = self.dataset_dir / class_name
                if not class_dataset_dir.exists():
                    self.logger.warning(f"âš ï¸ {class_name} ë°ì´í„°ì…‹ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                all_images = []
                for img_path in class_dataset_dir.rglob("*"):
                    if (img_path.is_file() and 
                        img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']):
                        all_images.append(img_path)
                
                self.logger.info(f"ğŸ“Š {class_name}: {len(all_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                
                if len(all_images) == 0:
                    continue
                
                # ëœë¤ ìƒ˜í”Œë§
                import random
                selected_count = min(images_per_class, len(all_images))
                selected_images = random.sample(all_images, selected_count)
                
                # ë³µì‚¬
                for i, img_path in enumerate(selected_images, 1):
                    try:
                        # ì›ë³¸ ê²½ë¡œì—ì„œ ì¢… ì •ë³´ ì¶”ì¶œ
                        relative_path = img_path.relative_to(class_dataset_dir)
                        species_info = "_".join(relative_path.parts[:-1])
                        
                        # ìƒˆ íŒŒì¼ëª… ìƒì„±
                        new_filename = f"{class_name}_{species_info}_{i:03d}{img_path.suffix}"
                        new_path = class_dir / new_filename
                        
                        import shutil
                        shutil.copy2(img_path, new_path)
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {img_path} - {e}")
                
                self.logger.info(f"âœ… {class_name}: {selected_count}ê°œ ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ")
            
            self.logger.info(f"ğŸ‰ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {train_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_dataset(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ë¶„ì„"""
        self.logger.info("ğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„ ì‹œì‘")
        
        analysis = {
            'total_images': 0,
            'class_distribution': {},
            'family_distribution': {},
            'species_distribution': {},
            'quality_analysis': {},
            'source_analysis': {},
            'file_size_analysis': {},
            'resolution_analysis': {}
        }
        
        try:
            if not self.dataset_dir.exists():
                self.logger.warning("âš ï¸ ë°ì´í„°ì…‹ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return analysis
            
            all_images = []
            
            # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
            for img_path in self.dataset_dir.rglob("*"):
                if (img_path.is_file() and 
                    img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']):
                    all_images.append(img_path)
            
            analysis['total_images'] = len(all_images)
            self.logger.info(f"ğŸ“Š ì´ {len(all_images)}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
            
            # í´ë˜ìŠ¤ë³„ ë¶„í¬
            for class_name in self.taxonomy_manager.fish_taxonomy.keys():
                class_dir = self.dataset_dir / class_name
                if class_dir.exists():
                    class_images = list(class_dir.rglob("*.jpg")) + list(class_dir.rglob("*.png"))
                    analysis['class_distribution'][class_name] = len(class_images)
            
            self.logger.info("âœ… ë°ì´í„°ì…‹ ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis
    
    def setup_auto_labeling_workspace(self) -> bool:
        """ì˜¤í†  ë¼ë²¨ë§ ì‘ì—…ê³µê°„ ì„¤ì •"""
        self.logger.info("ğŸ·ï¸ ì˜¤í†  ë¼ë²¨ë§ ì‘ì—…ê³µê°„ ì„¤ì • ì‹œì‘")
        
        try:
            # ë¼ë²¨ë§ ì‘ì—…ê³µê°„ í´ë” ìƒì„±
            labeling_dir = self.base_dir / "auto_labeling"
            labeling_dir.mkdir(exist_ok=True)
            
            # í•˜ìœ„ í´ë”ë“¤ ìƒì„±
            folders = [
                "unlabeled", "labeled", "verified", "rejected", 
                "models", "annotations", "exports"
            ]
            
            for folder in folders:
                (labeling_dir / folder).mkdir(exist_ok=True)
            
            # í´ë˜ìŠ¤ë³„ í•˜ìœ„ í´ë” ìƒì„±
            for main_folder in ["labeled", "verified"]:
                for class_name in self.taxonomy_manager.fish_taxonomy.keys():
                    (labeling_dir / main_folder / class_name).mkdir(exist_ok=True)
            
            self.logger.info(f"âœ… ì˜¤í†  ë¼ë²¨ë§ ì‘ì—…ê³µê°„ ì„¤ì • ì™„ë£Œ: {labeling_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜¤í†  ë¼ë²¨ë§ ì‘ì—…ê³µê°„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_scraping_statistics(self) -> Dict[str, Any]:
        """ìŠ¤í¬ë˜í•‘ í†µê³„ ë°˜í™˜"""
        stats = self.scraping_stats.copy()
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„ ì¶”ê°€
        download_stats = self.image_downloader.get_download_statistics()
        stats['download_stats'] = download_stats
        
        # ì—ëŸ¬ í†µê³„ ì¶”ê°€
        error_stats = self.error_handler.get_error_statistics()
        stats['error_stats'] = error_stats
        
        # ë©”íƒ€ë°ì´í„° í†µê³„ ì¶”ê°€
        if len(self.metadata_collection) > 0:
            metadata_stats = self.metadata_collection.get_statistics()
            stats['metadata_stats'] = metadata_stats
        
        return stats
    
    def export_metadata(self, file_path: str) -> bool:
        """ë©”íƒ€ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            self.metadata_collection.save_to_file(Path(file_path))
            self.logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ìŠ¤í¬ë˜í¼ë“¤ ì •ë¦¬
            for scraper in self.scrapers.values():
                if hasattr(scraper, 'close'):
                    scraper.close()
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë” ì •ë¦¬
            self.image_downloader.close()
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            self.image_downloader.cleanup_temp_files()
            
            self.logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup_resources()
    
    def __del__(self):
        try:
            self.cleanup_resources()
        except:
            pass