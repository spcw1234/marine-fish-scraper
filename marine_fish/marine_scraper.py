#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°„ë‹¨í•œ ê´€ìƒìš© í•´ìˆ˜ì–´ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ í”„ë¡œê·¸ë¨
Simple Marine Fish Image Scraper
"""

import os
import requests
import time
import json
import random
import shutil
import re
import hashlib
from pathlib import Path
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from collections import defaultdict
from datetime import datetime


class MarineScraper:
    """ê°„ë‹¨í•œ í•´ìˆ˜ì–´ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, config=None):
        self.base_dir = Path(".")
        self.dataset_dir = self.base_dir / "dataset"
        self.train_dir = self.base_dir / "train"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.dataset_dir.mkdir(exist_ok=True)
        self.train_dir.mkdir(exist_ok=True)
        
        # HTTP ì„¸ì…˜
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ì„¤ì • ì €ì¥
        self.config = config
        
        # í’ˆì§ˆ í†µê³„ ì¶”ê°€
        self.quality_stats = {
            'total_checked': 0,
            'quality_passed': 0,
            'quality_failed': 0,
            'blur_rejected': 0,
            'brightness_rejected': 0,
            'contrast_rejected': 0,
            'noise_rejected': 0,
            'resolution_rejected': 0
        }
        
        # í•´ìˆ˜ì–´ ë¶„ë¥˜ (ê°„ë‹¨í•œ ë²„ì „)
        self.fish_taxonomy = {
            "ì—°ê³¨ì–´ë¥˜": {
                "ìƒì–´ë¥˜": {
                    "Chiloscyllium": {
                        "punctatum": ["Brownbanded bamboo shark", "ê°ˆìƒ‰ì¤„ë¬´ëŠ¬ ëŒ€ë‚˜ë¬´ìƒì–´"]
                    }
                }
            },
            "ê²½ê³¨ì–´ë¥˜": {
                "ê°€ì‹œê³ ë“±ì–´ëª©": {
                    "Paracanthurus": {
                        "hepatus": ["Blue tang", "ë¸”ë£¨íƒ±", "ë„ë¦¬"]
                    },
                    "Zebrasoma": {
                        "flavescens": ["Yellow tang", "ì˜ë¡œìš°íƒ±"],
                        "xanthurum": ["Purple tang", "í¼í”Œíƒ±"]
                    },
                    "Centropyge": {
                        "bicolor": ["Bicolor angelfish", "ë°”ì´ì»¬ëŸ¬ ì—”ì ¤"],
                        "loricula": ["Flame angelfish", "í”Œë ˆì„ ì—”ì ¤"]
                    },
                    "Amphiprion": {
                        "ocellaris": ["Ocellaris clownfish", "ì˜¤ì…€ë¼ë¦¬ìŠ¤ í´ë¼ìš´", "ë‹ˆëª¨"],
                        "percula": ["Percula clownfish", "í¼í˜ë¼ í´ë¼ìš´"],
                        "clarkii": ["Clark's anemonefish", "í´ë½ìŠ¤ í´ë¼ìš´"]
                    },
                    "Chaetodon": {
                        "auriga": ["Threadfin butterflyfish", "ì‹¤ì§€ëŠëŸ¬ë¯¸ ë‚˜ë¹„ê³ ê¸°"],
                        "lunula": ["Raccoon butterflyfish", "ë¼ì¿¤ ë‚˜ë¹„ê³ ê¸°"]
                    }
                },
                "ë³µì–´ëª©": {
                    "Arothron": {
                        "nigropunctatus": ["Blackspotted puffer", "ê²€ì€ì  ë³µì–´"],
                        "meleagris": ["Guineafowl puffer", "ê¸°ë‹ˆíŒŒìš¸ ë³µì–´"]
                    },
                    "Rhinecanthus": {
                        "aculeatus": ["Lagoon triggerfish", "ë¼êµ° íŠ¸ë¦¬ê±°"],
                        "rectangulus": ["Wedgetail triggerfish", "ì›¨ì§€í…Œì¼ íŠ¸ë¦¬ê±°"]
                    }
                }
            }
        }
        
        # í†µê³„
        self.stats = {
            'total_downloaded': 0,
            'total_errors': 0
        }
    
    def download_image(self, url, save_path):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° í’ˆì§ˆ ê²€ì¦"""
        try:
            response = self.session.get(url, timeout=15, stream=True)
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '')
                
                if 'image' in content_type:
                    # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ë‹¤ìš´ë¡œë“œ
                    temp_path = save_path.with_suffix('.tmp')
                    with open(temp_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    
                    # ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦
                    if self.is_good_quality_image(temp_path):
                        # í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ ìµœì¢… íŒŒì¼ë¡œ ì´ë™
                        temp_path.rename(save_path)
                        return True
                    else:
                        # í’ˆì§ˆì´ ë‚˜ì˜ë©´ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        temp_path.unlink()
                        return False
            return False
        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} - {e}")
            return False
    
    def is_good_quality_image(self, image_path):
        """OpenCVë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦"""
        self.quality_stats['total_checked'] += 1
        
        try:
            import cv2
            import numpy as np
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = cv2.imread(str(image_path))
            if img is None:
                self.quality_stats['quality_failed'] += 1
                return False
            
            height, width = img.shape[:2]
            
            # 1. ìµœì†Œ í•´ìƒë„ ì²´í¬ (ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ ì œì™¸)
            if width < 200 or height < 200:
                self.quality_stats['resolution_rejected'] += 1
                self.quality_stats['quality_failed'] += 1
                return False
            
            # 2. ì¢…íš¡ë¹„ ì²´í¬ (ë„ˆë¬´ ê·¹ë‹¨ì ì¸ ë¹„ìœ¨ ì œì™¸)
            aspect_ratio = width / height
            if aspect_ratio < 0.3 or aspect_ratio > 3.0:
                self.quality_stats['quality_failed'] += 1
                return False
            
            # 3. ë¸”ëŸ¬ ê²€ì¶œ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            if laplacian_var < 100:  # ë¸”ëŸ¬ ì„ê³„ê°’
                self.quality_stats['blur_rejected'] += 1
                self.quality_stats['quality_failed'] += 1
                return False
            
            # 4. ë°ê¸° ì²´í¬ (ë„ˆë¬´ ì–´ë‘¡ê±°ë‚˜ ë°ì€ ì´ë¯¸ì§€ ì œì™¸)
            mean_brightness = np.mean(gray)
            if mean_brightness < 30 or mean_brightness > 225:
                self.quality_stats['brightness_rejected'] += 1
                self.quality_stats['quality_failed'] += 1
                return False
            
            # 5. ëŒ€ë¹„ ì²´í¬ (ëŒ€ë¹„ê°€ ë„ˆë¬´ ë‚®ì€ ì´ë¯¸ì§€ ì œì™¸)
            contrast = np.std(gray)
            if contrast < 20:
                self.quality_stats['contrast_rejected'] += 1
                self.quality_stats['quality_failed'] += 1
                return False
            
            # 6. ìƒ‰ìƒ ë‹¤ì–‘ì„± ì²´í¬ (ë‹¨ìƒ‰ ì´ë¯¸ì§€ ì œì™¸)
            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            color_std = np.std(hsv[:,:,1])  # ì±„ë„ì˜ í‘œì¤€í¸ì°¨
            if color_std < 15:
                self.quality_stats['quality_failed'] += 1
                return False
            
            # 7. ë…¸ì´ì¦ˆ ë ˆë²¨ ì²´í¬ (ê³¼ë„í•œ ë…¸ì´ì¦ˆ ì œì™¸)
            noise_level = self.estimate_noise_level(gray)
            if noise_level > 25:
                self.quality_stats['noise_rejected'] += 1
                self.quality_stats['quality_failed'] += 1
                return False
            
            # ëª¨ë“  ê²€ì‚¬ë¥¼ í†µê³¼í•˜ë©´ ì¢‹ì€ í’ˆì§ˆ
            self.quality_stats['quality_passed'] += 1
            return True
            
        except Exception as e:
            # OpenCV ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ê²€ì¦ë§Œ ìˆ˜í–‰
            return self.basic_image_validation(image_path)
    
    def estimate_noise_level(self, gray_img):
        """ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •"""
        try:
            import cv2
            import numpy as np
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš© í›„ ì›ë³¸ê³¼ì˜ ì°¨ì´ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
            blurred = cv2.GaussianBlur(gray_img, (5, 5), 0)
            noise = cv2.absdiff(gray_img, blurred)
            return np.mean(noise)
        except:
            return 0
    
    def basic_image_validation(self, image_path):
        """ê¸°ë³¸ ì´ë¯¸ì§€ ê²€ì¦ (OpenCV ì—†ì´)"""
        try:
            from PIL import Image
            with Image.open(image_path) as img:
                width, height = img.size
                
                # ìµœì†Œ í•´ìƒë„ ì²´í¬
                if width < 200 or height < 200:
                    return False
                
                # ì¢…íš¡ë¹„ ì²´í¬
                aspect_ratio = width / height
                if aspect_ratio < 0.3 or aspect_ratio > 3.0:
                    return False
                
                # íŒŒì¼ í¬ê¸° ì²´í¬ (ë„ˆë¬´ ì‘ìœ¼ë©´ í’ˆì§ˆì´ ë‚®ì„ ê°€ëŠ¥ì„±)
                file_size = image_path.stat().st_size
                if file_size < 10000:  # 10KB ë¯¸ë§Œ
                    return False
                
                return True
        except:
            return False
    
    def search_fishbase(self, genus, species, max_images=50):
        """FishBaseì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        images = []
        
        try:
            # FishBase URLë“¤
            urls = [
                f"https://www.fishbase.se/summary/{genus}-{species}.html",
                f"https://www.fishbase.se/photos/ThumbnailsSummary.php?Genus={genus}&Species={species}",
                f"https://www.fishbase.se/search.php?lang=English&SearchALL={genus}+{species}"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        img_tags = soup.find_all('img', src=True)
                        
                        for img_tag in img_tags:
                            img_url = img_tag.get('src')
                            if img_url and not img_url.startswith('http'):
                                img_url = urljoin(url, img_url)
                            
                            if self.is_valid_image_url(img_url):
                                images.append(img_url)
                                
                                if len(images) >= max_images:
                                    break
                    
                    time.sleep(1)  # ìš”ì²­ ê°„ ì§€ì—°
                    
                except Exception as e:
                    print(f"FishBase ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    continue
                    
                if len(images) >= max_images:
                    break
                    
        except Exception as e:
            print(f"FishBase ì „ì²´ ì˜¤ë¥˜: {e}")
        
        return images[:max_images]
    
    def search_google_images(self, genus, species, common_names=None, max_images=30):
        """Google Imagesì—ì„œ ê²€ìƒ‰"""
        images = []
        
        # ê²€ìƒ‰ì–´ ìƒì„±
        search_terms = [
            f"{genus} {species} fish",
            f"{species} fish"
        ]
        
        if common_names:
            for name in common_names[:2]:
                search_terms.append(f"{name} fish")
        
        for search_term in search_terms:
            try:
                search_url = f"https://www.google.com/search?q={quote(search_term)}&tbm=isch"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    # ê°„ë‹¨í•œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
                    img_urls = re.findall(r'"(https://[^"]*\.(?:jpg|jpeg|png|gif))"', response.text)
                    
                    for img_url in img_urls[:10]:  # ê²€ìƒ‰ì–´ë‹¹ ìµœëŒ€ 10ê°œ
                        if self.is_valid_image_url(img_url):
                            images.append(img_url)
                            
                            if len(images) >= max_images:
                                break
                
                time.sleep(2)  # ê²€ìƒ‰ì–´ ê°„ ì§€ì—°
                
            except Exception as e:
                print(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {search_term} - {e}")
                continue
                
            if len(images) >= max_images:
                break
        
        return images[:max_images]
    
    def search_google_images_extended(self, genus, species, common_names=None, max_images=150):
        """í™•ì¥ëœ Google Images ê²€ìƒ‰ (ë‹¤ì–‘í•œ ê°ë„ì™€ ìƒí™©)"""
        images = []
        
        # ë‹¤ì–‘í•œ ê°ë„ì™€ ìƒí™©ì„ ìœ„í•œ í™•ì¥ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ
        base_terms = [
            f"{genus} {species}",
            f"{genus} {species} fish",
            f"{genus} {species} marine",
            f"{genus} {species} aquarium",
            f"{genus} {species} underwater",
            f"{genus} {species} profile",
            f"{genus} {species} side view",
            f"{genus} {species} front view",
            f"{genus} {species} swimming",
            f"{genus} {species} coral reef",
            f"{genus} {species} different angles",
            f"{genus} {species} close up",
            f"{genus} {species} full body"
        ]
        
        if common_names:
            for name in common_names[:3]:  # ìƒìœ„ 3ê°œ ì¼ë°˜ëª… ì‚¬ìš©
                base_terms.extend([
                    name,
                    f"{name} fish",
                    f"{name} marine fish",
                    f"{name} aquarium fish",
                    f"{name} underwater photo",
                    f"{name} swimming",
                    f"{name} profile view",
                    f"{name} side angle",
                    f"{name} different poses",
                    f"{name} various angles"
                ])
        
        for term in base_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://www.google.com/search?q={quote(term)}&tbm=isch"
                response = self.session.get(search_url, timeout=20)
                
                if response.status_code == 200:
                    import re
                    img_urls = re.findall(r'"(https?://[^"]*\.(?:jpg|jpeg|png|gif))"', response.text)
                    
                    for url in img_urls[:6]:  # ê²€ìƒ‰ì–´ë‹¹ ìµœëŒ€ 6ê°œ
                        if len(images) >= max_images:
                            break
                        if self.is_valid_image_url(url):
                            images.append(url)
                
                time.sleep(0.4)  # ìš”ì²­ ê°„ ì§€ì—°
                
            except Exception as e:
                print(f"    Google ê²€ìƒ‰ ì˜¤ë¥˜ ({term}): {e}")
                continue
        
        return images[:max_images]
    
    def search_wikipedia(self, genus, species, common_names=None, max_images=30):
        """Wikipediaì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        images = []
        
        search_terms = [f"{genus} {species}", f"{genus}_{species}"]
        if common_names:
            search_terms.extend([name.replace(' ', '_') for name in common_names[:2]])
        
        for term in search_terms:
            if len(images) >= max_images:
                break
                
            try:
                wiki_url = f"https://en.wikipedia.org/wiki/{quote(term)}"
                response = self.session.get(wiki_url, timeout=15)
                
                if response.status_code == 200:
                    import re
                    # Wikipedia ì´ë¯¸ì§€ íŒ¨í„´
                    patterns = [
                        r'//upload\.wikimedia\.org/[^"]*\.(?:jpg|jpeg|png)',
                        r'https://upload\.wikimedia\.org/[^"]*\.(?:jpg|jpeg|png)'
                    ]
                    
                    for pattern in patterns:
                        urls = re.findall(pattern, response.text)
                        for url in urls:
                            if not url.startswith('http'):
                                url = f"https:{url}"
                            if self.is_valid_image_url(url):
                                images.append(url)
                                if len(images) >= max_images:
                                    break
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    Wikipedia ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return images[:max_images]
    
    def search_flickr(self, genus, species, common_names=None, max_images=40):
        """Flickrì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        images = []
        
        search_terms = [f"{genus} {species}", f"{genus} {species} fish"]
        if common_names:
            search_terms.extend(common_names[:2])
        
        for term in search_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://www.flickr.com/search/?text={quote(term)}"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    import re
                    # Flickr ì´ë¯¸ì§€ íŒ¨í„´
                    patterns = [
                        r'https://live\.staticflickr\.com/[^"]*_[a-z]\.jpg',
                        r'https://farm\d+\.staticflickr\.com/[^"]*\.jpg'
                    ]
                    
                    for pattern in patterns:
                        urls = re.findall(pattern, response.text)
                        for url in urls[:8]:
                            if self.is_valid_image_url(url):
                                images.append(url)
                                if len(images) >= max_images:
                                    break
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    Flickr ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return images[:max_images]
    
    def search_with_additional_terms(self, genus, species, common_names=None, max_images=50):
        """ì¶”ê°€ ê²€ìƒ‰ì–´ë¡œ ë” ë§ì€ ì´ë¯¸ì§€ ìˆ˜ì§‘"""
        images = []
        
        # í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ë‹¤ì–‘í•œ ìƒí™©ì˜ ê²€ìƒ‰ì–´
        additional_terms = [
            f"{genus} {species} juvenile",  # ì–´ë¦° ê°œì²´
            f"{genus} {species} adult",     # ì„±ì²´
            f"{genus} {species} male",      # ìˆ˜ì»·
            f"{genus} {species} female",    # ì•”ì»·
            f"{genus} {species} breeding",  # ë²ˆì‹ê¸°
            f"{genus} {species} natural habitat",  # ìì—° ì„œì‹ì§€
            f"{genus} {species} aquarium tank",    # ìˆ˜ì¡±ê´€
            f"{genus} {species} feeding",   # ë¨¹ì´ í™œë™
            f"{genus} {species} schooling", # ë¬´ë¦¬ í–‰ë™
            f"{genus} {species} behavior"   # í–‰ë™
        ]
        
        if common_names:
            for name in common_names[:2]:
                additional_terms.extend([
                    f"{name} juvenile",
                    f"{name} adult fish",
                    f"{name} natural environment",
                    f"{name} aquarium"
                ])
        
        for term in additional_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://www.google.com/search?q={quote(term)}&tbm=isch"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    import re
                    img_urls = re.findall(r'"(https?://[^"]*\.(?:jpg|jpeg|png|gif))"', response.text)
                    
                    for url in img_urls[:4]:  # ê²€ìƒ‰ì–´ë‹¹ ìµœëŒ€ 4ê°œ
                        if len(images) >= max_images:
                            break
                        if self.is_valid_image_url(url):
                            images.append(url)
                
                time.sleep(0.6)
                
            except Exception as e:
                print(f"    ì¶”ê°€ ê²€ìƒ‰ ì˜¤ë¥˜ ({term}): {e}")
                continue
        
        return images[:max_images]
    
    def search_bing_images(self, genus, species, common_names=None, max_images=120):
        """Bing Imagesì—ì„œ ê²€ìƒ‰"""
        images = []
        
        search_terms = [
            f"{genus} {species}",
            f"{genus} {species} fish",
            f"{genus} {species} marine aquarium",
            f"{genus} {species} underwater photography"
        ]
        
        if common_names:
            for name in common_names[:3]:
                search_terms.extend([
                    name,
                    f"{name} fish",
                    f"{name} marine fish",
                    f"{name} aquarium"
                ])
        
        for term in search_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://www.bing.com/images/search?q={quote(term)}"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    import re
                    # Bing ì´ë¯¸ì§€ URL íŒ¨í„´
                    img_urls = re.findall(r'"murl":"([^"]*\.(?:jpg|jpeg|png|gif))"', response.text)
                    
                    for url in img_urls[:8]:
                        if len(images) >= max_images:
                            break
                        if self.is_valid_image_url(url):
                            images.append(url)
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    Bing ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return images[:max_images]
    
    def search_yandex_images(self, genus, species, common_names=None, max_images=100):
        """Yandex Imagesì—ì„œ ê²€ìƒ‰"""
        images = []
        
        search_terms = [f"{genus} {species}", f"{genus} {species} fish"]
        if common_names:
            search_terms.extend(common_names[:2])
        
        for term in search_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://yandex.com/images/search?text={quote(term)}"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    import re
                    # Yandex ì´ë¯¸ì§€ íŒ¨í„´
                    img_urls = re.findall(r'"url":"([^"]*\.(?:jpg|jpeg|png|gif))"', response.text)
                    
                    for url in img_urls[:10]:
                        if len(images) >= max_images:
                            break
                        # URL ë””ì½”ë”©
                        url = url.replace('\\/', '/')
                        if self.is_valid_image_url(url):
                            images.append(url)
                
                time.sleep(0.7)
                
            except Exception as e:
                print(f"    Yandex ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return images[:max_images]
    
    def search_reef2reef(self, genus, species, common_names=None, max_images=40):
        """Reef2Reef í¬ëŸ¼ì—ì„œ ê²€ìƒ‰"""
        images = []
        
        search_terms = [f"{genus} {species}"]
        if common_names:
            search_terms.extend(common_names[:2])
        
        for term in search_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://www.reef2reef.com/search/1/?q={quote(term)}&o=relevance"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    img_tags = soup.find_all('img', src=True)
                    
                    for img in img_tags:
                        if len(images) >= max_images:
                            break
                        src = img.get('src')
                        if src and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png']):
                            if not src.startswith('http'):
                                src = urljoin('https://www.reef2reef.com', src)
                            if self.is_valid_image_url(src):
                                images.append(src)
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    Reef2Reef ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return images[:max_images]
    
    def search_marinespecies_org(self, genus, species, max_images=30):
        """WoRMS (World Register of Marine Species)ì—ì„œ ê²€ìƒ‰"""
        images = []
        
        try:
            # WoRMS API ê²€ìƒ‰
            search_url = f"http://www.marinespecies.org/rest/AphiaRecordsByName/{quote(genus + ' ' + species)}"
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                if data and len(data) > 0:
                    aphia_id = data[0].get('AphiaID')
                    if aphia_id:
                        # ì´ë¯¸ì§€ ê²€ìƒ‰
                        img_url = f"http://www.marinespecies.org/rest/AphiaExternalIDsByAphiaID/{aphia_id}?type=images"
                        img_response = self.session.get(img_url, timeout=15)
                        
                        if img_response.status_code == 200:
                            img_data = img_response.json()
                            for item in img_data[:max_images]:
                                if 'url' in item:
                                    url = item['url']
                                    if self.is_valid_image_url(url):
                                        images.append(url)
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    MarineSpecies.org ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return images[:max_images]
    
    def search_eol(self, genus, species, common_names=None, max_images=35):
        """Encyclopedia of Life (EOL)ì—ì„œ ê²€ìƒ‰"""
        images = []
        
        try:
            # EOL ê²€ìƒ‰
            search_url = f"https://eol.org/search?q={quote(genus + ' ' + species)}"
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ì´ë¯¸ì§€ ë§í¬ ì°¾ê¸°
                img_links = soup.find_all('a', href=True)
                for link in img_links:
                    if len(images) >= max_images:
                        break
                    href = link.get('href')
                    if href and '/media/' in href:
                        # ë¯¸ë””ì–´ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        try:
                            media_url = urljoin('https://eol.org', href)
                            media_response = self.session.get(media_url, timeout=10)
                            if media_response.status_code == 200:
                                media_soup = BeautifulSoup(media_response.content, 'html.parser')
                                img_tags = media_soup.find_all('img', src=True)
                                for img in img_tags:
                                    src = img.get('src')
                                    if src and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png']):
                                        if not src.startswith('http'):
                                            src = urljoin('https://eol.org', src)
                                        if self.is_valid_image_url(src):
                                            images.append(src)
                                            break
                        except:
                            continue
            
            time.sleep(1)
            
        except Exception as e:
            print(f"    EOL ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return images[:max_images]
    
    def search_inaturalist(self, genus, species, common_names=None, max_images=45):
        """iNaturalistì—ì„œ ê²€ìƒ‰"""
        images = []
        
        try:
            # iNaturalist API ê²€ìƒ‰
            search_url = f"https://api.inaturalist.org/v1/taxa?q={quote(genus + ' ' + species)}"
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                if data.get('results'):
                    taxon_id = data['results'][0].get('id')
                    if taxon_id:
                        # ê´€ì°° ì´ë¯¸ì§€ ê²€ìƒ‰
                        obs_url = f"https://api.inaturalist.org/v1/observations?taxon_id={taxon_id}&photos=true&per_page=30"
                        obs_response = self.session.get(obs_url, timeout=15)
                        
                        if obs_response.status_code == 200:
                            obs_data = obs_response.json()
                            for obs in obs_data.get('results', []):
                                if len(images) >= max_images:
                                    break
                                for photo in obs.get('photos', []):
                                    if len(images) >= max_images:
                                        break
                                    url = photo.get('url')
                                    if url:
                                        # ê³ í•´ìƒë„ ë²„ì „ URL ìƒì„±
                                        high_res_url = url.replace('square', 'large')
                                        if self.is_valid_image_url(high_res_url):
                                            images.append(high_res_url)
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    iNaturalist ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return images[:max_images]
    
    def search_gbif(self, genus, species, max_images=25):
        """GBIF (Global Biodiversity Information Facility)ì—ì„œ ê²€ìƒ‰"""
        images = []
        
        try:
            # GBIF ì¢… ê²€ìƒ‰
            search_url = f"https://api.gbif.org/v1/species/match?name={quote(genus + ' ' + species)}"
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                species_key = data.get('speciesKey')
                if species_key:
                    # ì´ë¯¸ì§€ê°€ ìˆëŠ” occurrence ê²€ìƒ‰
                    occ_url = f"https://api.gbif.org/v1/occurrence/search?speciesKey={species_key}&mediaType=StillImage&limit=20"
                    occ_response = self.session.get(occ_url, timeout=15)
                    
                    if occ_response.status_code == 200:
                        occ_data = occ_response.json()
                        for result in occ_data.get('results', []):
                            if len(images) >= max_images:
                                break
                            media = result.get('media', [])
                            for item in media:
                                if len(images) >= max_images:
                                    break
                                url = item.get('identifier')
                                if url and self.is_valid_image_url(url):
                                    images.append(url)
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    GBIF ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return images[:max_images]
    
    def search_aquarium_sites(self, genus, species, common_names=None, max_images=40):
        """ìˆ˜ì¡±ê´€ ê´€ë ¨ ì‚¬ì´íŠ¸ë“¤ì—ì„œ ê²€ìƒ‰"""
        images = []
        
        # ì£¼ìš” ìˆ˜ì¡±ê´€ ì‚¬ì´íŠ¸ë“¤
        aquarium_sites = [
            "liveaquaria.com",
            "marinedepot.com", 
            "saltwaterfish.com",
            "aquariumcoop.com"
        ]
        
        search_terms = [f"{genus} {species}"]
        if common_names:
            search_terms.extend(common_names[:2])
        
        for site in aquarium_sites:
            if len(images) >= max_images:
                break
                
            for term in search_terms:
                if len(images) >= max_images:
                    break
                    
                try:
                    # Google site-specific ê²€ìƒ‰
                    search_url = f"https://www.google.com/search?q=site:{site}+{quote(term)}&tbm=isch"
                    response = self.session.get(search_url, timeout=15)
                    
                    if response.status_code == 200:
                        import re
                        img_urls = re.findall(r'"(https?://[^"]*\.(?:jpg|jpeg|png|gif))"', response.text)
                        
                        for url in img_urls[:5]:  # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 5ê°œ
                            if len(images) >= max_images:
                                break
                            if site in url and self.is_valid_image_url(url):
                                images.append(url)
                    
                    time.sleep(0.8)
                    
                except Exception as e:
                    print(f"    {site} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    continue
        
        return images[:max_images]
    
    def search_pinterest(self, genus, species, common_names=None, max_images=35):
        """Pinterestì—ì„œ ê²€ìƒ‰"""
        images = []
        
        search_terms = [f"{genus} {species} fish"]
        if common_names:
            search_terms.extend([f"{name} fish" for name in common_names[:2]])
        
        for term in search_terms:
            if len(images) >= max_images:
                break
                
            try:
                search_url = f"https://www.pinterest.com/search/pins/?q={quote(term)}"
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    import re
                    # Pinterest ì´ë¯¸ì§€ íŒ¨í„´
                    img_urls = re.findall(r'"url":"([^"]*\.(?:jpg|jpeg|png))"', response.text)
                    
                    for url in img_urls[:12]:
                        if len(images) >= max_images:
                            break
                        url = url.replace('\\/', '/')
                        if self.is_valid_image_url(url):
                            images.append(url)
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    Pinterest ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return images[:max_images]
    
    def is_valid_image_url(self, url):
        """ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸"""
        if not url or len(url) < 20:
            return False
        
        # ì œì™¸í•  íŒ¨í„´
        exclude_patterns = ['icon', 'logo', 'banner', 'button', 'avatar']
        if any(pattern in url.lower() for pattern in exclude_patterns):
            return False
        
        # ì´ë¯¸ì§€ í™•ì¥ì ë˜ëŠ” í‚¤ì›Œë“œ í™•ì¸
        image_indicators = ['.jpg', '.jpeg', '.png', '.gif', 'fish', 'image', 'photo']
        if any(indicator in url.lower() for indicator in image_indicators):
            return True
        
        return False
    
    def scrape_species(self, genus, species, common_names=None, target_images=1000):
        """ë‹¨ì¼ ì¢… ìŠ¤í¬ë˜í•‘ (í™•ì¥ëœ ë‹¤ì¤‘ ì†ŒìŠ¤)"""
        print(f"\nğŸ” {genus} {species} ê²€ìƒ‰ ì¤‘...")
        
        # ì €ì¥ í´ë” ìƒì„±
        species_dir = self.dataset_dir / f"{genus}_{species}"
        species_dir.mkdir(exist_ok=True)
        
        # ì´ë¯¸ì§€ URL ìˆ˜ì§‘ (ë‹¤ì¤‘ ì†ŒìŠ¤)
        all_urls = []
        
        # 1. FishBaseì—ì„œ ìˆ˜ì§‘ (ì „ë¬¸ ì–´ë¥˜ DB)
        print("  ğŸ“¥ FishBase ê²€ìƒ‰ ì¤‘...")
        try:
            fishbase_urls = self.search_fishbase(genus, species, 100)
            all_urls.extend(fishbase_urls)
            print(f"  ğŸ“Š FishBase: {len(fishbase_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ FishBase ì˜¤ë¥˜: {e}")
        
        # 2. Google Imagesì—ì„œ ìˆ˜ì§‘ (í™•ì¥ëœ ê²€ìƒ‰ì–´)
        print("  ğŸ“¥ Google Images ê²€ìƒ‰ ì¤‘...")
        try:
            google_urls = self.search_google_images_extended(genus, species, common_names, 150)
            all_urls.extend(google_urls)
            print(f"  ğŸ“Š Google: {len(google_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Google ì˜¤ë¥˜: {e}")
        
        # 3. Bing Imagesì—ì„œ ìˆ˜ì§‘ (ë§ˆì´í¬ë¡œì†Œí”„íŠ¸)
        print("  ğŸ“¥ Bing Images ê²€ìƒ‰ ì¤‘...")
        try:
            bing_urls = self.search_bing_images(genus, species, common_names, 120)
            all_urls.extend(bing_urls)
            print(f"  ğŸ“Š Bing: {len(bing_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Bing ì˜¤ë¥˜: {e}")
        
        # 4. Yandex Imagesì—ì„œ ìˆ˜ì§‘ (ëŸ¬ì‹œì•„ ê²€ìƒ‰ì—”ì§„)
        print("  ğŸ“¥ Yandex Images ê²€ìƒ‰ ì¤‘...")
        try:
            yandex_urls = self.search_yandex_images(genus, species, common_names, 100)
            all_urls.extend(yandex_urls)
            print(f"  ğŸ“Š Yandex: {len(yandex_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Yandex ì˜¤ë¥˜: {e}")
        
        # 5. iNaturalistì—ì„œ ìˆ˜ì§‘ (ìƒë¬¼ ê´€ì°° í”Œë«í¼)
        print("  ğŸ“¥ iNaturalist ê²€ìƒ‰ ì¤‘...")
        try:
            inaturalist_urls = self.search_inaturalist(genus, species, common_names, 80)
            all_urls.extend(inaturalist_urls)
            print(f"  ğŸ“Š iNaturalist: {len(inaturalist_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ iNaturalist ì˜¤ë¥˜: {e}")
        
        # 6. Flickrì—ì„œ ìˆ˜ì§‘ (ì‚¬ì§„ ê³µìœ )
        print("  ğŸ“¥ Flickr ê²€ìƒ‰ ì¤‘...")
        try:
            flickr_urls = self.search_flickr(genus, species, common_names, 70)
            all_urls.extend(flickr_urls)
            print(f"  ğŸ“Š Flickr: {len(flickr_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Flickr ì˜¤ë¥˜: {e}")
        
        # 7. ìˆ˜ì¡±ê´€ ì‚¬ì´íŠ¸ë“¤ì—ì„œ ìˆ˜ì§‘ (ìƒì—…ì  ì‚¬ì´íŠ¸)
        print("  ğŸ“¥ ìˆ˜ì¡±ê´€ ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì¤‘...")
        try:
            aquarium_urls = self.search_aquarium_sites(genus, species, common_names, 80)
            all_urls.extend(aquarium_urls)
            print(f"  ğŸ“Š ìˆ˜ì¡±ê´€ ì‚¬ì´íŠ¸: {len(aquarium_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ ìˆ˜ì¡±ê´€ ì‚¬ì´íŠ¸ ì˜¤ë¥˜: {e}")
        
        # 8. Pinterestì—ì„œ ìˆ˜ì§‘ (ì´ë¯¸ì§€ ê³µìœ )
        print("  ğŸ“¥ Pinterest ê²€ìƒ‰ ì¤‘...")
        try:
            pinterest_urls = self.search_pinterest(genus, species, common_names, 70)
            all_urls.extend(pinterest_urls)
            print(f"  ğŸ“Š Pinterest: {len(pinterest_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Pinterest ì˜¤ë¥˜: {e}")
        
        # 9. Encyclopedia of Lifeì—ì„œ ìˆ˜ì§‘ (ìƒë¬¼ ë°±ê³¼ì‚¬ì „)
        print("  ğŸ“¥ EOL ê²€ìƒ‰ ì¤‘...")
        try:
            eol_urls = self.search_eol(genus, species, common_names, 60)
            all_urls.extend(eol_urls)
            print(f"  ğŸ“Š EOL: {len(eol_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ EOL ì˜¤ë¥˜: {e}")
        
        # 10. Wikipediaì—ì„œ ìˆ˜ì§‘ (ë°±ê³¼ì‚¬ì „)
        print("  ğŸ“¥ Wikipedia ê²€ìƒ‰ ì¤‘...")
        try:
            wiki_urls = self.search_wikipedia(genus, species, common_names, 50)
            all_urls.extend(wiki_urls)
            print(f"  ğŸ“Š Wikipedia: {len(wiki_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Wikipedia ì˜¤ë¥˜: {e}")
        
        # 11. GBIFì—ì„œ ìˆ˜ì§‘ (ê¸€ë¡œë²Œ ìƒë¬¼ë‹¤ì–‘ì„±)
        print("  ğŸ“¥ GBIF ê²€ìƒ‰ ì¤‘...")
        try:
            gbif_urls = self.search_gbif(genus, species, 40)
            all_urls.extend(gbif_urls)
            print(f"  ğŸ“Š GBIF: {len(gbif_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ GBIF ì˜¤ë¥˜: {e}")
        
        # 12. WoRMSì—ì„œ ìˆ˜ì§‘ (í•´ì–‘ìƒë¬¼ ë“±ë¡ì†Œ)
        print("  ğŸ“¥ WoRMS ê²€ìƒ‰ ì¤‘...")
        try:
            worms_urls = self.search_marinespecies_org(genus, species, 50)
            all_urls.extend(worms_urls)
            print(f"  ğŸ“Š WoRMS: {len(worms_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ WoRMS ì˜¤ë¥˜: {e}")
        
        # 13. Reef2Reef í¬ëŸ¼ì—ì„œ ìˆ˜ì§‘ (í•´ìˆ˜ì–´ ì»¤ë®¤ë‹ˆí‹°)
        print("  ğŸ“¥ Reef2Reef ê²€ìƒ‰ ì¤‘...")
        try:
            reef2reef_urls = self.search_reef2reef(genus, species, common_names, 60)
            all_urls.extend(reef2reef_urls)
            print(f"  ğŸ“Š Reef2Reef: {len(reef2reef_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ Reef2Reef ì˜¤ë¥˜: {e}")
        
        # 14. ì¶”ê°€ ê²€ìƒ‰ì–´ë¡œ Google ì¬ê²€ìƒ‰ (íŠ¹ìˆ˜ ìƒí™©)
        print("  ğŸ“¥ ì¶”ê°€ ê²€ìƒ‰ì–´ë¡œ ì¬ê²€ìƒ‰ ì¤‘...")
        try:
            additional_urls = self.search_with_additional_terms(genus, species, common_names, 100)
            all_urls.extend(additional_urls)
            print(f"  ğŸ“Š ì¶”ê°€ ê²€ìƒ‰: {len(additional_urls)}ê°œ ë°œê²¬")
        except Exception as e:
            print(f"  âŒ ì¶”ê°€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        unique_urls = []
        seen_urls = set()
        
        for url in all_urls:
            if url not in seen_urls and self.is_valid_image_url(url):
                # ë¬¸ì œê°€ ìˆëŠ” ì„œë²„ ìŠ¤í‚µ
                if any(problem_server in url for problem_server in ['mediaphoto.mnhn.fr', 'timeout-prone-server.com']):
                    continue
                unique_urls.append(url)
                seen_urls.add(url)
        
        print(f"  ğŸ¯ ì´ {len(unique_urls)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ ë°œê²¬ (í•„í„°ë§ í›„)")
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤)
        downloaded = 0
        failed = 0
        
        for i, url in enumerate(unique_urls[:target_images]):
            filename = f"{genus}_{species}_{i+1:03d}.jpg"
            save_path = species_dir / filename
            
            try:
                if self.download_image(url, save_path):
                    downloaded += 1
                    if downloaded % 20 == 0:
                        print(f"    ğŸ“¥ {downloaded}ì¥ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ...")
                else:
                    failed += 1
            except Exception as e:
                failed += 1
                if failed % 10 == 0:
                    print(f"    âš ï¸ {failed}ê°œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨...")
            
            # ì ì‘ì  ì§€ì—° (ì„±ê³µë¥ ì— ë”°ë¼ ì¡°ì •)
            success_rate = downloaded / (downloaded + failed) if (downloaded + failed) > 0 else 1
            delay = 0.3 if success_rate > 0.8 else 0.8 if success_rate > 0.5 else 1.5
            time.sleep(delay)
        
        # í’ˆì§ˆ í†µê³„ ê³„ì‚°
        quality_rate = (self.quality_stats['quality_passed'] / max(1, self.quality_stats['total_checked'])) * 100
        
        print(f"  âœ… {genus} {species}: {downloaded}ì¥ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        print(f"     ğŸ“Š í’ˆì§ˆ í†µê³¼ìœ¨: {quality_rate:.1f}% ({self.quality_stats['quality_passed']}/{self.quality_stats['total_checked']})")
        if self.quality_stats['quality_failed'] > 0:
            print(f"     ğŸš« í’ˆì§ˆ ë¶ˆëŸ‰: ë¸”ëŸ¬({self.quality_stats['blur_rejected']}) ë°ê¸°({self.quality_stats['brightness_rejected']}) ëŒ€ë¹„({self.quality_stats['contrast_rejected']}) ë…¸ì´ì¦ˆ({self.quality_stats['noise_rejected']}) í•´ìƒë„({self.quality_stats['resolution_rejected']})")
        
        self.stats['total_downloaded'] += downloaded
        self.stats['total_errors'] += failed
        
        # í’ˆì§ˆ í†µê³„ ì´ˆê¸°í™” (ë‹¤ìŒ ì¢…ì„ ìœ„í•´)
        self.quality_stats = {
            'total_checked': 0,
            'quality_passed': 0,
            'quality_failed': 0,
            'blur_rejected': 0,
            'brightness_rejected': 0,
            'contrast_rejected': 0,
            'noise_rejected': 0,
            'resolution_rejected': 0
        }
        
        return downloaded
    
    def scrape_all_fish(self):
        """ëª¨ë“  ë¬¼ê³ ê¸° ëŒ€ëŸ‰ ìŠ¤í¬ë˜í•‘ (ML í•™ìŠµìš©)"""
        print("ğŸ  í•´ìˆ˜ì–´ ì´ë¯¸ì§€ ëŒ€ëŸ‰ ìŠ¤í¬ë˜í•‘ ì‹œì‘!")
        print(f"ğŸ“Š ì´ {self.count_species()}ì¢… ì²˜ë¦¬ ì˜ˆì •")
        print("ğŸ¯ ì¢…ë‹¹ ëª©í‘œ: 1000ì¥ (ëŒ€ëŸ‰ ìˆ˜ì§‘ í›„ ê³ í’ˆì§ˆ ì„ ë³„)")
        print("ğŸ” 14ê°œ ì†ŒìŠ¤: FishBase, Google, Wikipedia, Flickr, Bing, Yandex,")
        print("             iNaturalist, EOL, GBIF, WoRMS, Reef2Reef, ìˆ˜ì¡±ê´€ì‚¬ì´íŠ¸, Pinterest")
        print("ğŸ“Š ì „ëµ: ëŒ€ëŸ‰ ìˆ˜ì§‘ â†’ í’ˆì§ˆ í•„í„°ë§ â†’ ì˜¤í†  ë¼ë²¨ë§ â†’ ML í•™ìŠµ")
        
        total_species = 0
        
        for class_name, orders in self.fish_taxonomy.items():
            print(f"\nğŸ“ {class_name} ì²˜ë¦¬ ì¤‘...")
            
            for order_name, genera in orders.items():
                print(f"  ğŸ“‚ {order_name}")
                
                for genus_name, species_dict in genera.items():
                    for species_name, common_names in species_dict.items():
                        total_species += 1
                        try:
                            print(f"\nğŸ”„ [{total_species}/{self.count_species()}] {genus_name} {species_name}")
                            downloaded = self.scrape_species(genus_name, species_name, common_names, 1000)
                            
                            # ì„±ê³µë¥ ì— ë”°ë¥¸ ì ì‘ì  ì§€ì—° (1000ì¥ ê¸°ì¤€)
                            if downloaded > 800:
                                print(f"  ğŸ‰ ìš°ìˆ˜í•œ ìˆ˜ì§‘ë¥ : {downloaded}ì¥ (80%+)")
                                time.sleep(1)  # ì„±ê³µì ì´ë©´ ì§§ì€ ì§€ì—°
                            elif downloaded > 600:
                                print(f"  âœ… ì–‘í˜¸í•œ ìˆ˜ì§‘ë¥ : {downloaded}ì¥ (60%+)")
                                time.sleep(2)  # ë³´í†µì´ë©´ ì¤‘ê°„ ì§€ì—°
                            elif downloaded > 400:
                                print(f"  âš ï¸ ë³´í†µ ìˆ˜ì§‘ë¥ : {downloaded}ì¥ (40%+)")
                                time.sleep(3)  # ë‚®ìœ¼ë©´ ê¸´ ì§€ì—°
                            elif downloaded > 200:
                                print(f"  âŒ ë‚®ì€ ìˆ˜ì§‘ë¥ : {downloaded}ì¥ (20%+)")
                                time.sleep(4)  # ë§¤ìš° ë‚®ìœ¼ë©´ ë” ê¸´ ì§€ì—°
                            else:
                                print(f"  ğŸ’¥ ë§¤ìš° ë‚®ì€ ìˆ˜ì§‘ë¥ : {downloaded}ì¥ (<20%)")
                                time.sleep(5)  # ê·¹ë„ë¡œ ë‚®ìœ¼ë©´ ê°€ì¥ ê¸´ ì§€ì—°
                                
                        except Exception as e:
                            print(f"  ğŸ’¥ {genus_name} {species_name} ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
                            self.stats['total_errors'] += 1
                            time.sleep(5)  # ì˜¤ë¥˜ ì‹œ ê¸´ ì§€ì—°
        
        # ìµœì¢… í†µê³„
        avg_per_species = self.stats['total_downloaded'] / total_species if total_species > 0 else 0
        success_rate = (total_species - self.stats['total_errors']) / total_species * 100 if total_species > 0 else 0
        
        print(f"\nğŸ‰ ëŒ€ëŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ: {self.stats['total_downloaded']:,}ì¥")
        print(f"ğŸ  ì²˜ë¦¬ëœ ì¢…: {total_species}ì¢…")
        print(f"ğŸ“ˆ í‰ê·  ì¢…ë‹¹: {avg_per_species:.1f}ì¥")
        print(f"âœ… ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"âš ï¸ ì´ ì˜¤ë¥˜: {self.stats['total_errors']}ê°œ")
        print("=" * 60)
    
    def create_training_dataset(self, images_per_class=50):
        """í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±"""
        print(f"ğŸ¯ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± (í´ë˜ìŠ¤ë‹¹ {images_per_class}ì¥)")
        
        # í´ë˜ìŠ¤ë³„ í´ë” ìƒì„±
        for class_name in self.fish_taxonomy.keys():
            class_dir = self.train_dir / class_name
            class_dir.mkdir(exist_ok=True)
            
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘
            all_images = []
            for img_path in self.dataset_dir.rglob("*.jpg"):
                all_images.append(img_path)
            
            if len(all_images) == 0:
                print(f"  âš ï¸ {class_name}: ì´ë¯¸ì§€ ì—†ìŒ")
                continue
            
            # ëœë¤ ìƒ˜í”Œë§
            selected_count = min(images_per_class, len(all_images))
            selected_images = random.sample(all_images, selected_count)
            
            # ë³µì‚¬
            for i, img_path in enumerate(selected_images, 1):
                new_name = f"{class_name}_{i:03d}.jpg"
                new_path = class_dir / new_name
                shutil.copy2(img_path, new_path)
            
            print(f"  âœ… {class_name}: {selected_count}ì¥ ë³µì‚¬ ì™„ë£Œ")
        
        print("ğŸ‰ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    
    def analyze_dataset(self):
        """ë°ì´í„°ì…‹ ë¶„ì„"""
        print("ğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘...")
        
        total_images = 0
        species_stats = {}
        
        for species_dir in self.dataset_dir.iterdir():
            if species_dir.is_dir():
                images = list(species_dir.glob("*.jpg"))
                count = len(images)
                species_stats[species_dir.name] = count
                total_images += count
        
        print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
        print(f"  ì´ ì´ë¯¸ì§€: {total_images}ì¥")
        print(f"  ì´ ì¢… ìˆ˜: {len(species_stats)}ì¢…")
        
        if species_stats:
            avg_images = total_images / len(species_stats)
            print(f"  í‰ê·  ì¢…ë‹¹ ì´ë¯¸ì§€: {avg_images:.1f}ì¥")
            
            # ìƒìœ„ 5ì¢…
            top_species = sorted(species_stats.items(), key=lambda x: x[1], reverse=True)[:5]
            print(f"\nğŸ† ì´ë¯¸ì§€ê°€ ë§ì€ ìƒìœ„ 5ì¢…:")
            for species, count in top_species:
                print(f"    {species}: {count}ì¥")
    
    def count_species(self):
        """ì´ ì¢… ìˆ˜ ê³„ì‚°"""
        count = 0
        for class_name, orders in self.fish_taxonomy.items():
            for order_name, genera in orders.items():
                for genus_name, species_dict in genera.items():
                    count += len(species_dict)
        return count
    
    # ê³ ê¸‰ CLI ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ë©”ì„œë“œë“¤
    def restore_session(self, session_id: str) -> bool:
        """ì„¸ì…˜ ë³µì› (ì„ì‹œ êµ¬í˜„)"""
        print(f"ì„¸ì…˜ ë³µì› ì‹œë„: {session_id}")
        # ì‹¤ì œë¡œëŠ” session_managerë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        return False
    
    def start_scraping_session(self, target_species, images_per_species: int):
        """ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ì‹œì‘"""
        print(f"ìŠ¤í¬ë˜í•‘ ì„¸ì…˜ ì‹œì‘: {len(target_species)}ì¢…, ì¢…ë‹¹ {images_per_species}ì¥")
        self.target_species = target_species
        self.images_per_species = images_per_species
    
    def run_scraping(self) -> dict:
        """ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print("ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ì¤‘...")
        
        total_downloaded = 0
        species_processed = 0
        
        if hasattr(self, 'target_species'):
            for species_info in self.target_species:
                genus = species_info.genus
                species = species_info.species
                common_names = species_info.common_names
                
                downloaded = self.scrape_species(genus, species, common_names, self.images_per_species)
                total_downloaded += downloaded
                species_processed += 1
        else:
            # ê¸°ë³¸ ë™ì‘: ëª¨ë“  ì¢… ìŠ¤í¬ë˜í•‘
            self.scrape_all_fish()
            total_downloaded = self.stats.get('total_downloaded', 0)
            species_processed = self.stats.get('species_processed', 0)
        
        return {
            'total_downloaded': total_downloaded,
            'duration': 0,  # ì‹¤ì œë¡œëŠ” ì‹œê°„ ì¸¡ì • í•„ìš”
            'species_processed': species_processed
        }
    
    def get_statistics(self) -> dict:
        """í†µê³„ ë°˜í™˜"""
        return getattr(self, 'stats', {
            'total_downloaded': 0,
            'total_errors': 0,
            'species_processed': 0
        })


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    scraper = MarineScraper()
    
    while True:
        print("\n" + "="*60)
        print("ğŸ  ê³ ì„±ëŠ¥ í•´ìˆ˜ì–´ ì´ë¯¸ì§€ ëŒ€ëŸ‰ ìŠ¤í¬ë˜í¼ v2.0")
        print("="*60)
        print("1. ì „ì²´ ì´ë¯¸ì§€ ëŒ€ëŸ‰ ë‹¤ìš´ë¡œë“œ (ì¢…ë‹¹ 1000ì¥ ëª©í‘œ)")
        print("2. í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± (ê³ í’ˆì§ˆ ì„ ë³„)")
        print("3. ë°ì´í„°ì…‹ ë¶„ì„ ë° í†µê³„")
        print("4. ì¢…ë£Œ")
        print("-"*60)
        print("ğŸ’¡ 14ê°œ ì†ŒìŠ¤ì—ì„œ ëŒ€ëŸ‰ ìˆ˜ì§‘ â†’ OpenCV í’ˆì§ˆ í•„í„°ë§ â†’ ì˜¤í†  ë¼ë²¨ë§")
        print("ğŸ” í’ˆì§ˆ ê²€ì‚¬: í•´ìƒë„, ë¸”ëŸ¬, ë°ê¸°, ëŒ€ë¹„, ìƒ‰ìƒ, ë…¸ì´ì¦ˆ")
        print("-"*60)
        
        choice = input("ì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
        
        if choice == '1':
            scraper.scrape_all_fish()
        
        elif choice == '2':
            try:
                images_per_class = input("í´ë˜ìŠ¤ë‹¹ ì´ë¯¸ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 500): ").strip()
                if images_per_class:
                    images_per_class = int(images_per_class)
                else:
                    images_per_class = 500
                
                scraper.create_training_dataset(images_per_class)
            except ValueError:
                print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        elif choice == '3':
            scraper.analyze_dataset()
        
        elif choice == '4':
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()